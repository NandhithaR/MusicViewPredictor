{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pymongo\n",
    "from datetime import datetime, timedelta\n",
    "from pymongo import InsertOne, DeleteOne, ReplaceOne, UpdateOne\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from pandas.io.json import json_normalize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing track data from the database\n",
    "client = pymongo.MongoClient(\"mongodb+srv://\" + \"vidit23\" + \":\" + \"dsba123\" + \"@mvp-bvqf2.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "connectedDB = client['MVP']\n",
    "songCollectionName = \"Videos\"\n",
    "artistCollectionName = \"Artists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of incoming data (39241, 166)\n"
     ]
    }
   ],
   "source": [
    "# Getting the data from the DB, check if useDate is present in the records fetched\n",
    "useDate = '24/04/2020'\n",
    "query_result = list(connectedDB[songCollectionName].find({'youtubeId': {'$exists': 1}, \n",
    "                                                          'views.' + useDate: {'$exists': 1}}))\n",
    "\n",
    "# Flatten the structure of the dataframe due to nested objects in database\n",
    "initialSongsDf = pd.json_normalize(query_result)\n",
    "print(\"Shape of incoming data\", initialSongsDf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching all artist information\n",
    "artistResult = list(connectedDB[artistCollectionName].find({}))\n",
    "artistDictionary = {artist['_id'] : [artist['popularity'], artist['followers']] for artist in artistResult}\n",
    "\n",
    "# Function to coalesce multiple artist into one\n",
    "def combineMultipleArtistInfo(row):\n",
    "    popularities = []\n",
    "    followers = []\n",
    "    for songArtist in row['artists']:\n",
    "        popularities.append(artistDictionary[songArtist['id']][0])\n",
    "        followers.append(artistDictionary[songArtist['id']][1])\n",
    "    return popularities, followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding corresponding artist for each track\n",
    "artistInfoDf = initialSongsDf.apply(lambda x: combineMultipleArtistInfo(x), axis=1, result_type='expand')\n",
    "artistInfoDf.columns = ['artistsPopularity', 'artistsFollowers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different combining functions based on artists\n",
    "artistInfoDf['artistPopularitySum'] = artistInfoDf['artistsPopularity'].apply(np.sum)\n",
    "artistInfoDf['artistPopularityMax'] = artistInfoDf['artistsPopularity'].apply(np.max)\n",
    "artistInfoDf['artistFollowerSum'] = artistInfoDf['artistsFollowers'].apply(np.sum)\n",
    "artistInfoDf['artistFollowerMax'] = artistInfoDf['artistsFollowers'].apply(np.max)\n",
    "artistInfoDf['numArtists'] = artistInfoDf['artistsPopularity'].apply(np.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining track and artist information\n",
    "initialSongsDf = initialSongsDf.merge(artistInfoDf,left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numDays = 2\n",
    "musicFeatureColumns = ['energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', \n",
    "                       'instrumentalness', 'liveness', 'valence', 'tempo', 'danceability']\n",
    "\n",
    "artistRelatedColumns = ['artistPopularitySum', 'artistPopularityMax','numArtists','artistFollowerSum','artistFollowerMax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after selecting the essential columns and dropping null values  (38677, 27)\n"
     ]
    }
   ],
   "source": [
    "# Dropping the unnecessary columns of previous and future days\n",
    "useDateFormatted = datetime.strptime(useDate, \"%d/%m/%Y\")\n",
    "viewsColumns = []\n",
    "engagementRelatedColumns = []\n",
    "\n",
    "for addDay in range(numDays+1):\n",
    "    deltaDay = (useDateFormatted + timedelta(days=addDay)).strftime('%d/%m/%Y')\n",
    "    viewsColumns += ['views.' + deltaDay + '.viewCount']\n",
    "    engagementRelatedColumns += [['views.' + deltaDay + '.spotifyPopularity', \n",
    "                                  'views.' + deltaDay + '.likeCount', \n",
    "                                  'views.' + deltaDay + '.dislikeCount', \n",
    "                                  'views.' + deltaDay + '.commentCount']]\n",
    "\n",
    "# Removing the target day related engagement data\n",
    "engagementRelatedColumns = engagementRelatedColumns[:-1]\n",
    "engagementRelatedColumnsFlattened = [item for sublist in engagementRelatedColumns for item in sublist]\n",
    "\n",
    "essentialSongsDf = initialSongsDf[musicFeatureColumns + artistRelatedColumns + \n",
    "                                  engagementRelatedColumnsFlattened + viewsColumns]\n",
    "\n",
    "# Dropping all the null rows with null values\n",
    "essentialSongsDf = essentialSongsDf.dropna()\n",
    "\n",
    "print(\"Shape after selecting the essential columns and dropping null values \", essentialSongsDf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after selecting the essential columns and dropping null values  (38677, 27)\n"
     ]
    }
   ],
   "source": [
    "# Renaming all the columns that have dates in them to standard names\n",
    "viewsMapping = { col: ('day' + str(index) + '.' + col.split('.')[-1]) for index, col in enumerate(viewsColumns)}\n",
    "engagementMapping = { colName: 'day' + str(index) + '.' + colName.split('.')[-1] for index, cols in enumerate(engagementRelatedColumns) for colName in cols }\n",
    "essentialSongsDf = essentialSongsDf.rename(columns = viewsMapping)\n",
    "essentialSongsDf = essentialSongsDf.rename(columns = engagementMapping)\n",
    "print(\"Shape after selecting the essential columns and dropping null values \", essentialSongsDf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the columns to type float64\n",
    "changeType = {column: 'float64' for column in list(set(essentialSongsDf.columns))}\n",
    "essentialSongsDf = essentialSongsDf.astype(changeType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping 0 values  (37237, 27)\n"
     ]
    }
   ],
   "source": [
    "# This remove rows where viewCount might be 0 or if views decrease from one day to the next\n",
    "essentialSongsDf = essentialSongsDf[(essentialSongsDf['day0.viewCount'] > 0) & \n",
    "                                    (essentialSongsDf['day1.viewCount'] > 0) & \n",
    "                                    (essentialSongsDf['day2.viewCount'] > 0) & \n",
    "                                    (essentialSongsDf['day2.viewCount'] > essentialSongsDf['day1.viewCount']) & \n",
    "                                    (essentialSongsDf['day1.viewCount'] > essentialSongsDf['day0.viewCount'])]\n",
    "print(\"Shape after dropping 0 values \", essentialSongsDf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes percentage change in view between (day1, day0) and (day2, day1)\n",
    "essentialSongsDf['first_percentage_increase'] = ((essentialSongsDf['day1.viewCount'] - essentialSongsDf['day0.viewCount']) * 100) / essentialSongsDf['day0.viewCount']\n",
    "essentialSongsDf['target_percentage_increase'] = ((essentialSongsDf['day2.viewCount'] - essentialSongsDf['day1.viewCount']) * 100) / essentialSongsDf['day1.viewCount']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping 0 values  (35899, 28)\n"
     ]
    }
   ],
   "source": [
    "essentialSongsDf = essentialSongsDf.drop('day2.viewCount', 1)\n",
    "essentialSongsDf = essentialSongsDf[(essentialSongsDf['day1.likeCount'] >= essentialSongsDf['day0.likeCount']) & \n",
    "                                    (essentialSongsDf['day1.dislikeCount'] >= essentialSongsDf['day0.dislikeCount'])]\n",
    "print(\"Shape after dropping 0 values \", essentialSongsDf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>...</th>\n",
       "      <th>day1.spotifyPopularity</th>\n",
       "      <th>day1.likeCount</th>\n",
       "      <th>day1.dislikeCount</th>\n",
       "      <th>day1.commentCount</th>\n",
       "      <th>day0.viewCount</th>\n",
       "      <th>day1.viewCount</th>\n",
       "      <th>first_percentage_increase</th>\n",
       "      <th>target_percentage_increase</th>\n",
       "      <th>likes_percentage_change</th>\n",
       "      <th>dislikes_percentage_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>3.589900e+04</td>\n",
       "      <td>3.589900e+04</td>\n",
       "      <td>3.589900e+04</td>\n",
       "      <td>3.589900e+04</td>\n",
       "      <td>3.589900e+04</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35872.000000</td>\n",
       "      <td>34273.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.697502</td>\n",
       "      <td>5.276832</td>\n",
       "      <td>-6.867439</td>\n",
       "      <td>0.627928</td>\n",
       "      <td>0.081929</td>\n",
       "      <td>0.187556</td>\n",
       "      <td>0.152315</td>\n",
       "      <td>0.197205</td>\n",
       "      <td>0.457866</td>\n",
       "      <td>123.798968</td>\n",
       "      <td>...</td>\n",
       "      <td>40.533775</td>\n",
       "      <td>1.126564e+05</td>\n",
       "      <td>5.491630e+03</td>\n",
       "      <td>5.749967e+03</td>\n",
       "      <td>1.945073e+07</td>\n",
       "      <td>1.945901e+07</td>\n",
       "      <td>0.219421</td>\n",
       "      <td>0.194813</td>\n",
       "      <td>0.192243</td>\n",
       "      <td>0.183266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.216979</td>\n",
       "      <td>3.568690</td>\n",
       "      <td>3.788229</td>\n",
       "      <td>0.483364</td>\n",
       "      <td>0.083243</td>\n",
       "      <td>0.260572</td>\n",
       "      <td>0.296682</td>\n",
       "      <td>0.158692</td>\n",
       "      <td>0.236822</td>\n",
       "      <td>29.362916</td>\n",
       "      <td>...</td>\n",
       "      <td>16.778016</td>\n",
       "      <td>5.665849e+05</td>\n",
       "      <td>7.444594e+04</td>\n",
       "      <td>5.025014e+04</td>\n",
       "      <td>1.141455e+08</td>\n",
       "      <td>1.141794e+08</td>\n",
       "      <td>0.930198</td>\n",
       "      <td>0.696499</td>\n",
       "      <td>0.868945</td>\n",
       "      <td>2.236361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-39.194000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.543000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.554000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-8.469500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.006890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096600</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>100.033000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>7.430000e+02</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>2.600000e+01</td>\n",
       "      <td>6.256800e+04</td>\n",
       "      <td>6.276350e+04</td>\n",
       "      <td>0.031644</td>\n",
       "      <td>0.029616</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.736000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-6.021000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>123.528000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>4.764000e+03</td>\n",
       "      <td>1.160000e+02</td>\n",
       "      <td>1.870000e+02</td>\n",
       "      <td>4.715650e+05</td>\n",
       "      <td>4.724460e+05</td>\n",
       "      <td>0.066752</td>\n",
       "      <td>0.062460</td>\n",
       "      <td>0.051129</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.878000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-4.352000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.075450</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.635000</td>\n",
       "      <td>143.842000</td>\n",
       "      <td>...</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>3.265350e+04</td>\n",
       "      <td>8.025000e+02</td>\n",
       "      <td>1.284000e+03</td>\n",
       "      <td>3.759686e+06</td>\n",
       "      <td>3.761030e+06</td>\n",
       "      <td>0.184767</td>\n",
       "      <td>0.171730</td>\n",
       "      <td>0.135596</td>\n",
       "      <td>0.022331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.999000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.893000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>222.605000</td>\n",
       "      <td>...</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>3.720587e+07</td>\n",
       "      <td>1.126088e+07</td>\n",
       "      <td>5.040935e+06</td>\n",
       "      <td>6.730952e+09</td>\n",
       "      <td>6.732749e+09</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>58.823529</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             energy           key      loudness          mode   speechiness  \\\n",
       "count  35899.000000  35899.000000  35899.000000  35899.000000  35899.000000   \n",
       "mean       0.697502      5.276832     -6.867439      0.627928      0.081929   \n",
       "std        0.216979      3.568690      3.788229      0.483364      0.083243   \n",
       "min        0.000943      0.000000    -39.194000      0.000000      0.022400   \n",
       "25%        0.554000      2.000000     -8.469500      0.000000      0.035500   \n",
       "50%        0.736000      5.000000     -6.021000      1.000000      0.049400   \n",
       "75%        0.878000      8.000000     -4.352000      1.000000      0.087900   \n",
       "max        0.999000     11.000000      1.893000      1.000000      0.953000   \n",
       "\n",
       "       acousticness  instrumentalness      liveness       valence  \\\n",
       "count  35899.000000      35899.000000  35899.000000  35899.000000   \n",
       "mean       0.187556          0.152315      0.197205      0.457866   \n",
       "std        0.260572          0.296682      0.158692      0.236822   \n",
       "min        0.000000          0.000000      0.013000      0.000000   \n",
       "25%        0.006890          0.000000      0.096600      0.272000   \n",
       "50%        0.056600          0.000102      0.131000      0.445000   \n",
       "75%        0.269000          0.075450      0.262000      0.635000   \n",
       "max        0.996000          0.997000      1.000000      1.000000   \n",
       "\n",
       "              tempo  ...  day1.spotifyPopularity  day1.likeCount  \\\n",
       "count  35899.000000  ...            35899.000000    3.589900e+04   \n",
       "mean     123.798968  ...               40.533775    1.126564e+05   \n",
       "std       29.362916  ...               16.778016    5.665849e+05   \n",
       "min       34.543000  ...                0.000000    0.000000e+00   \n",
       "25%      100.033000  ...               30.000000    7.430000e+02   \n",
       "50%      123.528000  ...               42.000000    4.764000e+03   \n",
       "75%      143.842000  ...               52.000000    3.265350e+04   \n",
       "max      222.605000  ...               96.000000    3.720587e+07   \n",
       "\n",
       "       day1.dislikeCount  day1.commentCount  day0.viewCount  day1.viewCount  \\\n",
       "count       3.589900e+04       3.589900e+04    3.589900e+04    3.589900e+04   \n",
       "mean        5.491630e+03       5.749967e+03    1.945073e+07    1.945901e+07   \n",
       "std         7.444594e+04       5.025014e+04    1.141455e+08    1.141794e+08   \n",
       "min         0.000000e+00       0.000000e+00    3.000000e+00    5.000000e+00   \n",
       "25%         1.700000e+01       2.600000e+01    6.256800e+04    6.276350e+04   \n",
       "50%         1.160000e+02       1.870000e+02    4.715650e+05    4.724460e+05   \n",
       "75%         8.025000e+02       1.284000e+03    3.759686e+06    3.761030e+06   \n",
       "max         1.126088e+07       5.040935e+06    6.730952e+09    6.732749e+09   \n",
       "\n",
       "       first_percentage_increase  target_percentage_increase  \\\n",
       "count               35899.000000                35899.000000   \n",
       "mean                    0.219421                    0.194813   \n",
       "std                     0.930198                    0.696499   \n",
       "min                     0.000558                    0.000049   \n",
       "25%                     0.031644                    0.029616   \n",
       "50%                     0.066752                    0.062460   \n",
       "75%                     0.184767                    0.171730   \n",
       "max                    66.666667                   58.823529   \n",
       "\n",
       "       likes_percentage_change  dislikes_percentage_change  \n",
       "count             35872.000000                34273.000000  \n",
       "mean                  0.192243                    0.183266  \n",
       "std                   0.868945                    2.236361  \n",
       "min                   0.000000                    0.000000  \n",
       "25%                   0.004180                    0.000000  \n",
       "50%                   0.051129                    0.000000  \n",
       "75%                   0.135596                    0.022331  \n",
       "max                  50.000000                  200.000000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percent increase in likes\n",
    "essentialSongsDf['likes_percentage_change'] = ((essentialSongsDf['day1.likeCount'] - essentialSongsDf['day0.likeCount']) * 100) / essentialSongsDf['day0.likeCount']\n",
    "\n",
    "# Percent increase in dislikes\n",
    "essentialSongsDf['dislikes_percentage_change'] = ((essentialSongsDf['day1.dislikeCount'] - essentialSongsDf['day0.dislikeCount']) * 100) / essentialSongsDf['day0.dislikeCount']\n",
    "# Converting all inf and nan values to 0\n",
    "essentialSongsDf = essentialSongsDf.replace([np.inf, -np.inf], 0)\n",
    "essentialSongsDf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viditb/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:3942: RuntimeWarning: invalid value encountered in multiply\n",
      "  x2 = take(ap, indices_above, axis=axis) * weights_above\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>...</th>\n",
       "      <th>day1.spotifyPopularity</th>\n",
       "      <th>log.day1.likeCount</th>\n",
       "      <th>log.day1.dislikeCount</th>\n",
       "      <th>log.day1.commentCount</th>\n",
       "      <th>log.day0.viewCount</th>\n",
       "      <th>log.day1.viewCount</th>\n",
       "      <th>log.first_percentage_increase</th>\n",
       "      <th>log.target_percentage_increase</th>\n",
       "      <th>log.likes_percentage_change</th>\n",
       "      <th>log.dislikes_percentage_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>3.589900e+04</td>\n",
       "      <td>3.589900e+04</td>\n",
       "      <td>3.589900e+04</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>35899.000000</td>\n",
       "      <td>3.587200e+04</td>\n",
       "      <td>3.427300e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.697502</td>\n",
       "      <td>5.276832</td>\n",
       "      <td>-6.867439</td>\n",
       "      <td>0.627928</td>\n",
       "      <td>0.081929</td>\n",
       "      <td>0.187556</td>\n",
       "      <td>0.152315</td>\n",
       "      <td>0.197205</td>\n",
       "      <td>0.457866</td>\n",
       "      <td>123.798968</td>\n",
       "      <td>...</td>\n",
       "      <td>40.533775</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>13.103697</td>\n",
       "      <td>13.105853</td>\n",
       "      <td>-2.546631</td>\n",
       "      <td>-2.620064</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.216979</td>\n",
       "      <td>3.568690</td>\n",
       "      <td>3.788229</td>\n",
       "      <td>0.483364</td>\n",
       "      <td>0.083243</td>\n",
       "      <td>0.260572</td>\n",
       "      <td>0.296682</td>\n",
       "      <td>0.158692</td>\n",
       "      <td>0.236822</td>\n",
       "      <td>29.362916</td>\n",
       "      <td>...</td>\n",
       "      <td>16.778016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.946287</td>\n",
       "      <td>2.944222</td>\n",
       "      <td>1.294576</td>\n",
       "      <td>1.285066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-39.194000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.543000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>-7.490644</td>\n",
       "      <td>-9.918253</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.554000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-8.469500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.006890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096600</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>100.033000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>6.610696e+00</td>\n",
       "      <td>2.833213e+00</td>\n",
       "      <td>3.258097e+00</td>\n",
       "      <td>11.044009</td>\n",
       "      <td>11.047129</td>\n",
       "      <td>-3.453197</td>\n",
       "      <td>-3.519433</td>\n",
       "      <td>-5.477435e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.736000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-6.021000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>123.528000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>8.468843e+00</td>\n",
       "      <td>4.753590e+00</td>\n",
       "      <td>5.231109e+00</td>\n",
       "      <td>13.063812</td>\n",
       "      <td>13.065679</td>\n",
       "      <td>-2.706765</td>\n",
       "      <td>-2.773237</td>\n",
       "      <td>-2.973399e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.878000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-4.352000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.075450</td>\n",
       "      <td>0.262000</td>\n",
       "      <td>0.635000</td>\n",
       "      <td>143.842000</td>\n",
       "      <td>...</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.039371e+01</td>\n",
       "      <td>6.687732e+00</td>\n",
       "      <td>7.157735e+00</td>\n",
       "      <td>15.139846</td>\n",
       "      <td>15.140203</td>\n",
       "      <td>-1.688659</td>\n",
       "      <td>-1.761829</td>\n",
       "      <td>-1.998079e+00</td>\n",
       "      <td>-3.801762e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.999000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.893000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>222.605000</td>\n",
       "      <td>...</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>1.743198e+01</td>\n",
       "      <td>1.623684e+01</td>\n",
       "      <td>1.543310e+01</td>\n",
       "      <td>22.629982</td>\n",
       "      <td>22.630249</td>\n",
       "      <td>4.199705</td>\n",
       "      <td>4.074542</td>\n",
       "      <td>3.912023e+00</td>\n",
       "      <td>5.298317e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             energy           key      loudness          mode   speechiness  \\\n",
       "count  35899.000000  35899.000000  35899.000000  35899.000000  35899.000000   \n",
       "mean       0.697502      5.276832     -6.867439      0.627928      0.081929   \n",
       "std        0.216979      3.568690      3.788229      0.483364      0.083243   \n",
       "min        0.000943      0.000000    -39.194000      0.000000      0.022400   \n",
       "25%        0.554000      2.000000     -8.469500      0.000000      0.035500   \n",
       "50%        0.736000      5.000000     -6.021000      1.000000      0.049400   \n",
       "75%        0.878000      8.000000     -4.352000      1.000000      0.087900   \n",
       "max        0.999000     11.000000      1.893000      1.000000      0.953000   \n",
       "\n",
       "       acousticness  instrumentalness      liveness       valence  \\\n",
       "count  35899.000000      35899.000000  35899.000000  35899.000000   \n",
       "mean       0.187556          0.152315      0.197205      0.457866   \n",
       "std        0.260572          0.296682      0.158692      0.236822   \n",
       "min        0.000000          0.000000      0.013000      0.000000   \n",
       "25%        0.006890          0.000000      0.096600      0.272000   \n",
       "50%        0.056600          0.000102      0.131000      0.445000   \n",
       "75%        0.269000          0.075450      0.262000      0.635000   \n",
       "max        0.996000          0.997000      1.000000      1.000000   \n",
       "\n",
       "              tempo  ...  day1.spotifyPopularity  log.day1.likeCount  \\\n",
       "count  35899.000000  ...            35899.000000        3.589900e+04   \n",
       "mean     123.798968  ...               40.533775                -inf   \n",
       "std       29.362916  ...               16.778016                 NaN   \n",
       "min       34.543000  ...                0.000000                -inf   \n",
       "25%      100.033000  ...               30.000000        6.610696e+00   \n",
       "50%      123.528000  ...               42.000000        8.468843e+00   \n",
       "75%      143.842000  ...               52.000000        1.039371e+01   \n",
       "max      222.605000  ...               96.000000        1.743198e+01   \n",
       "\n",
       "       log.day1.dislikeCount  log.day1.commentCount  log.day0.viewCount  \\\n",
       "count           3.589900e+04           3.589900e+04        35899.000000   \n",
       "mean                    -inf                   -inf           13.103697   \n",
       "std                      NaN                    NaN            2.946287   \n",
       "min                     -inf                   -inf            1.098612   \n",
       "25%             2.833213e+00           3.258097e+00           11.044009   \n",
       "50%             4.753590e+00           5.231109e+00           13.063812   \n",
       "75%             6.687732e+00           7.157735e+00           15.139846   \n",
       "max             1.623684e+01           1.543310e+01           22.629982   \n",
       "\n",
       "       log.day1.viewCount  log.first_percentage_increase  \\\n",
       "count        35899.000000                   35899.000000   \n",
       "mean            13.105853                      -2.546631   \n",
       "std              2.944222                       1.294576   \n",
       "min              1.609438                      -7.490644   \n",
       "25%             11.047129                      -3.453197   \n",
       "50%             13.065679                      -2.706765   \n",
       "75%             15.140203                      -1.688659   \n",
       "max             22.630249                       4.199705   \n",
       "\n",
       "       log.target_percentage_increase  log.likes_percentage_change  \\\n",
       "count                    35899.000000                 3.587200e+04   \n",
       "mean                        -2.620064                         -inf   \n",
       "std                          1.285066                          NaN   \n",
       "min                         -9.918253                         -inf   \n",
       "25%                         -3.519433                -5.477435e+00   \n",
       "50%                         -2.773237                -2.973399e+00   \n",
       "75%                         -1.761829                -1.998079e+00   \n",
       "max                          4.074542                 3.912023e+00   \n",
       "\n",
       "       log.dislikes_percentage_change  \n",
       "count                    3.427300e+04  \n",
       "mean                             -inf  \n",
       "std                               NaN  \n",
       "min                              -inf  \n",
       "25%                               NaN  \n",
       "50%                               NaN  \n",
       "75%                     -3.801762e+00  \n",
       "max                      5.298317e+00  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking the log of the skewed data and changing column names accordingly\n",
    "logList = ['day0.likeCount','day0.dislikeCount','day0.commentCount','day1.likeCount','day1.dislikeCount',\n",
    "           'day1.commentCount','day0.viewCount','day1.viewCount','artistFollowerSum','artistFollowerMax', \n",
    "           'first_percentage_increase', 'target_percentage_increase', 'likes_percentage_change', 'dislikes_percentage_change']\n",
    "\n",
    "logMapping = { col: ('log' +  '.' + col) for index, col in enumerate(logList)}\n",
    "essentialSongsDf = essentialSongsDf.rename(columns = logMapping)\n",
    "\n",
    "logCols = logMapping.values()\n",
    "for cols in logCols: \n",
    "    essentialSongsDf[cols] = essentialSongsDf[cols].apply(lambda x: np.log(x))\n",
    "\n",
    "essentialSongsDf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "essentialSongsDf = essentialSongsDf.replace([np.inf, -np.inf, np.nan], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the features and the label\n",
    "predictor_columns = ['energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness',\n",
    "       'instrumentalness', 'liveness', 'valence', 'tempo', 'danceability',\n",
    "       'artistPopularitySum', 'artistPopularityMax', 'numArtists',\n",
    "       'log.artistFollowerSum', 'log.artistFollowerMax',\n",
    "       'day0.spotifyPopularity', 'log.day0.likeCount', 'log.day0.dislikeCount',\n",
    "       'log.day0.commentCount', 'day1.spotifyPopularity', 'log.day1.likeCount',\n",
    "       'log.day1.dislikeCount', 'log.day1.commentCount', 'log.day0.viewCount',\n",
    "       'log.day1.viewCount', 'log.first_percentage_increase', 'log.likes_percentage_change',\n",
    "       'log.dislikes_percentage_change']\n",
    "target_columns = 'log.target_percentage_increase'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "x_scaled = robust_scaler.fit_transform(essentialSongsDf)\n",
    "df_test_robust = pd.DataFrame(x_scaled, columns=essentialSongsDf.columns)\n",
    "X_train_robust, X_test_robust, Y_train_robust, Y_test_robust = train_test_split(df_test_robust[predictor_columns], df_test_robust[target_columns], test_size=0.2, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Rate Mean Squared Error for Robust Scaler 0.029055551257308097\n",
      "Base Rate R Squared Error for Robust Scaler 0.9448472831144035\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "X_test_robust_df = pd.DataFrame(X_test_robust)\n",
    "print('Base Rate Mean Squared Error for Robust Scaler', mean_squared_error(Y_test_robust, X_test_robust_df[['log.first_percentage_increase']]))\n",
    "print('Base Rate R Squared Error for Robust Scaler', r2_score(Y_test_robust, X_test_robust_df[['log.first_percentage_increase']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "df_test_minmax = essentialSongsDf.copy()\n",
    "x = df_test_minmax.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()#feature_range can be set\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df_test_minmax = pd.DataFrame(x_scaled)\n",
    "column_names = essentialSongsDf.columns\n",
    "df_test_minmax.columns = column_names\n",
    "X_train_min_max, X_test_min_max, Y_train_min_max, Y_test_min_max = train_test_split(df_test_minmax[predictor_columns], df_test_minmax[target_columns], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Rate Mean Squared Error for MinMax Scaler 0.01057798940513532\n",
      "Base Rate R Squared Error for MinMax Scaler -0.27264691205699365\n"
     ]
    }
   ],
   "source": [
    "X_test_minmax_df = pd.DataFrame(X_test_min_max)\n",
    "print('Base Rate Mean Squared Error for MinMax Scaler', mean_squared_error(Y_test_min_max, X_test_minmax_df[['log.first_percentage_increase']]))\n",
    "print('Base Rate R Squared Error for MinMax Scaler', r2_score(Y_test_min_max, X_test_minmax_df[['log.first_percentage_increase']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_samples_leaf(X_train, Y_train):\n",
    "    min_samples_leaf = [100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000]\n",
    "    param_grid = dict(min_samples_leaf=min_samples_leaf)\n",
    "    decisionTreeRegressor = DecisionTreeRegressor(min_samples_leaf=100)\n",
    "    grid = GridSearchCV(estimator=decisionTreeRegressor, param_grid=param_grid, \n",
    "                    scoring='neg_mean_squared_error', verbose=1, cv=10)\n",
    "    grid_result = grid.fit(X_train, Y_train)\n",
    "    print('Best Score: ', grid_result.best_score_)\n",
    "    print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_model(X_train, Y_train, X_test, Y_test, X, Y, min_samples_leaf = 100):\n",
    "    decisionTreeRegressor = DecisionTreeRegressor(min_samples_leaf=min_samples_leaf)\n",
    "    decisionTreeRegressor.fit(X_train, Y_train)\n",
    "    preds = decisionTreeRegressor.predict(X_test)\n",
    "    test_score = decisionTreeRegressor.score(X_test, Y_test)\n",
    "    print('Test Score', test_score)\n",
    "    train_score = decisionTreeRegressor.score(X_train, Y_train)\n",
    "    print('Train Score',train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:   34.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  -0.028407421763437652\n",
      "Best Params:  {'min_samples_leaf': 100}\n",
      "Test Score 0.946403228017084\n",
      "Train Score 0.9517064897145575\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# using Grid search CV technique, optimum value fot min_sample_leaf is found. \n",
    "find_min_samples_leaf(X_train_robust, Y_train_robust)\n",
    "decision_model(X_train_robust, Y_train_robust, X_test_robust, Y_test_robust, df_test_robust[predictor_columns], df_test_robust[target_columns], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:   35.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  -0.0004480903882860731\n",
      "Best Params:  {'min_samples_leaf': 100}\n",
      "Test Score 0.9464098889139747\n",
      "Train Score 0.9517065412809771\n"
     ]
    }
   ],
   "source": [
    "find_min_samples_leaf(X_train_min_max, Y_train_min_max)\n",
    "decision_model(X_train_min_max, Y_train_min_max, X_test_min_max, Y_test_min_max, df_test_minmax[predictor_columns], df_test_minmax[target_columns], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors_parameters(X_train, Y_train):\n",
    "    n_neighbors = [100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000]\n",
    "    param_grid = dict(n_neighbors=n_neighbors)\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors, weights='distance')\n",
    "    grid = GridSearchCV(estimator=knn, param_grid=param_grid, \n",
    "                    scoring='neg_mean_squared_error', verbose=1, cv=10)\n",
    "    grid_result = grid.fit(X_train, Y_train)\n",
    "    print('Best Score for distance: ', grid_result.best_score_)\n",
    "    print('Best Params for distance: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbor_model(X_train, Y_train, X_test, Y_test, X, Y, weights = 'uniform', n_neighbors = 5):\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors, weights)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    preds = knn.predict(X_test)\n",
    "    test_score = knn.score(X_test, Y_test)\n",
    "    print('Test Score', test_score)\n",
    "    train_score = knn.score(X_train, Y_train)\n",
    "    print('Train Score',train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:  8.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score for distance:  -0.16209540682730633\n",
      "Best Params for distance:  {'n_neighbors': 100}\n",
      "Test Score 0.6980009428636405\n",
      "Train Score 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "find_neighbors_parameters(X_train_robust, Y_train_robust)\n",
    "neighbor_model(X_train_robust, Y_train_robust, X_test_robust, Y_test_robust, df_test_robust[predictor_columns], df_test_robust[target_columns], 'distance', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:  8.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score for distance:  -0.0035302093618191296\n",
      "Best Params for distance:  {'n_neighbors': 100}\n",
      "Test Score 0.5814419068480756\n",
      "Train Score 1.0\n"
     ]
    }
   ],
   "source": [
    "find_neighbors_parameters(X_train_min_max, Y_train_min_max)\n",
    "neighbor_model(X_train_min_max, Y_train_min_max, X_test_min_max, Y_test_min_max, df_test_minmax[predictor_columns], df_test_minmax[target_columns], 'distance', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ridge_alpha(X_train, Y_train): \n",
    "    # find optimal alpha with grid search\n",
    "    alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    param_grid = dict(alpha=alpha)\n",
    "\n",
    "    ridge = Ridge(alpha=1)\n",
    "    grid = GridSearchCV(estimator=ridge, param_grid=param_grid, \n",
    "                    scoring='neg_mean_squared_error', verbose=1, cv=10)\n",
    "    grid_result = grid.fit(X_train, Y_train)\n",
    "    print('Best Score: ', grid_result.best_score_)\n",
    "    print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_model(X_train, Y_train, X_test, Y_test, X, Y, alpha_value = 1):\n",
    "    ridge = Ridge(alpha=alpha_value)\n",
    "    ridge.fit(X_train, Y_train)\n",
    "    preds = ridge.predict(X_test)\n",
    "    test_score = ridge.score(X_test, Y_test)\n",
    "    print('Test Score', test_score)\n",
    "    train_score = ridge.score(X_train, Y_train)\n",
    "    print('Train Score',train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  -0.027471048102805005\n",
      "Best Params:  {'alpha': 0.01}\n",
      "Test Score 0.946596426244818\n",
      "Train Score 0.948935684950028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "find_ridge_alpha(X_train_robust, Y_train_robust)\n",
    "ridge_model(X_train_robust, Y_train_robust, X_test_robust, Y_test_robust, df_test_robust[predictor_columns], df_test_robust[target_columns], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n",
      "Best Score:  -0.0004334286310378959\n",
      "Best Params:  {'alpha': 0.001}\n",
      "Test Score 0.9466769696225609\n",
      "Train Score 0.9489533304703122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "find_ridge_alpha(X_train_min_max, Y_train_min_max)\n",
    "ridge_model(X_train_min_max, Y_train_min_max, X_test_min_max, Y_test_min_max, df_test_minmax[predictor_columns], df_test_minmax[target_columns], 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lasso_alpha(X_train, Y_train):\n",
    "    # find optimal alpha with grid search\n",
    "    alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    param_grid = dict(alpha=alpha)\n",
    "    lasso = Lasso(alpha=1)\n",
    "    grid_lasso = GridSearchCV(estimator=lasso, param_grid=param_grid, \n",
    "                    scoring='neg_mean_squared_error', verbose=1, cv=10)\n",
    "    grid_lasso_result = grid_lasso.fit(X_train, Y_train)\n",
    "    print('Best Score: ', grid_lasso_result.best_score_)\n",
    "    print('Best Params: ', grid_lasso_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_model(X_train, Y_train, X_test, Y_test, X, Y, alpha_value = 1):\n",
    "    lasso = Lasso(alpha=alpha_value)\n",
    "    lasso.fit(X_train, Y_train)\n",
    "    preds = lasso.predict(X_test)\n",
    "    test_score = lasso.score(X_test, Y_test)\n",
    "    print('Test Score', test_score)\n",
    "    train_score = lasso.score(X_train, Y_train)\n",
    "    print('Train Score',train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n",
      "Best Score:  -0.027512765811517615\n",
      "Best Params:  {'alpha': 0.001}\n",
      "Test Score 0.9464739740417091\n",
      "Train Score 0.9487951637238837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "find_lasso_alpha(X_train_robust, Y_train_robust)\n",
    "lasso_model(X_train_robust, Y_train_robust, X_test_robust, Y_test_robust, df_test_robust[predictor_columns], df_test_robust[target_columns], 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n",
      "Best Score:  -0.0005191946782293205\n",
      "Best Params:  {'alpha': 0.001}\n",
      "Test Score 0.9379982599138168\n",
      "Train Score 0.938675471419076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "find_lasso_alpha(X_train_min_max, Y_train_min_max)\n",
    "lasso_model(X_train_min_max, Y_train_min_max, X_test_min_max, Y_test_min_max, df_test_minmax[predictor_columns], df_test_minmax[target_columns], 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X_train, Y_train, X_test, Y_test, X, Y):\n",
    "    linear_regressor = LinearRegression() \n",
    "    linear_regressor.fit(X_train, Y_train)\n",
    "    preds = linear_regressor.predict(X_test)\n",
    "    test_score = linear_regressor.score(X_test, Y_test)\n",
    "    print('Test Score', test_score)\n",
    "    train_score = linear_regressor.score(X_train, Y_train)\n",
    "    print('Train Score',train_score)\n",
    "    print('Root Mean Square Error', mean_squared_error(Y_test_min_max, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score 0.9467752397651785\n",
      "Train Score 0.9489627906242687\n",
      "Root Mean Square Error 0.5709739598110675\n"
     ]
    }
   ],
   "source": [
    "linear_model(X_train_robust, Y_train_robust, X_test_robust, Y_test_robust, df_test_robust[predictor_columns], df_test_robust[target_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score 0.9467752397651786\n",
      "Train Score 0.9489627906242687\n",
      "Root Mean Square Error 0.0004423936792843881\n"
     ]
    }
   ],
   "source": [
    "linear_model(X_train_min_max, Y_train_min_max, X_test_min_max, Y_test_min_max, df_test_minmax[predictor_columns], df_test_minmax[target_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
